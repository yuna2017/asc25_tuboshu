% !TEX program = xelatex
\documentclass[a4paper,12pt]{article}

% Essential packages
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xeCJK}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{float}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{lastpage}

% Page geometry
\geometry{
    top=25mm,
    bottom=25mm,
    left=25mm,
    right=25mm
}

% Header and footer style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}

% Title formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Code listing style
\lstset{
    basicstyle=\small\ttfamily,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true
}

% Document info
\title{
    \includegraphics[width=0.4\textwidth]{ysu-logo.png}\\[1cm] % logo
    {\LARGE Proposal For ASC 25}\\[0.5cm]
    \large Team Name: \texttt{[Team Name]}\\[1cm] % 使用占位符
    \begin{tabular}{c}
        \large Team Member 1 \\
        \large Team Member 2 \\
        \large Team Member 3 \\
        \large Team Member 4 \\
        \large Team Member 5
    \end{tabular}
}
\author{}
\date{\vfill January 8, 2025}

\begin{document}

\maketitle
\thispagestyle{empty}
\vfill
\begin{center}
    \textit{A proposal submitted to the ASC 25 committee}
\end{center}
\newpage

\tableofcontents
\addtocontents{toc}{\protect\enlargethispage{\baselineskip}}
\newpage

\section{Brief Background Description of Supercomputing Activities}

\subsection{Hardware and Software Platforms}
Our university established a high-performance computational (HPC) cluster in 2025 to address the growing demands in scientific research and industrial applications. Here's an example of how to include technical specifications:

\begin{table}[H]
\centering
\caption{Hardware Configuration of YSU HPC Cluster}
\begin{tabular}{llll}
\toprule
Item & Name & Configuration & Number \\
\midrule
Login Node & [Model] & CPU: [Specs] & 1 \\
Compute Node & [Model] & CPU: [Specs] & 10 \\
GPU Node & [Model] & GPU: [Specs] & 2 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Example Code Block}
Here's how to include code samples in the document:

\begin{lstlisting}[language=Python, caption=HPL Performance Testing Script]
def test_hpl_performance(problem_size, block_size):
    """
    Test HPL performance with given parameters
    """
    results = []
    for size in problem_size:
        perf = run_hpl_benchmark(size, block_size)
        results.append((size, perf))
    return results
\end{lstlisting}

\subsection{Example Figure}
Example of including a figure with caption:

\begin{figure}[H]
\centering
% 可以插入图像文件，也可以使用 LaTeX 绘图包生成图像
% \includegraphics[width=0.8\textwidth]{example-performance-graph.png}
This is a placeholder for the performance graph
\caption{HPL Performance Scaling Analysis}
\label{fig:performance}
\end{figure}

\newpage

\section{Design of HPC System}

\subsection{Performance Analysis}
Example of including mathematical equations:

\begin{equation}
R_{peak} = N_{cores} \times N_{flops/cycle} \times F_{clock}
\end{equation}

Where:
\begin{itemize}
    \item \(N_{cores}\) is the total number of CPU cores
    \item \(N_{flops/cycle}\) is the number of floating-point operations per cycle
    \item \(F_{clock}\) is the clock frequency in Hz
\end{itemize}

\newpage

\section{Introduction to the University's Activities in Supercomputing}

\subsection{Supercomputing-related Hardware and Software Platforms}
% ...existing content...

\subsection{Supercomputing-related Courses, Trainings, and Interest Groups}
% 在这里添加关于课程、培训和兴趣小组的描述

\subsection{Supercomputing-related Research and Applications}
% 在这里添加关于研究和应用的描述

\subsection{Key Achievements in Supercomputing Research}
% 在这里简要描述不超过两项的关键成就

\newpage

\section{Team Introduction}

\subsection{Team Setup}
% 在这里简要描述团队设置

\subsection{Team Members}
% 在这里介绍每个成员并附上照片，包括团队合影

\subsection{Team Motto}
% 在这里添加团队的座右铭或口号

\newpage

\section{Technical Proposal Requirements}

\subsection{Design of HPC System}

\subsubsection{Theoretical Design of an HPC Cluster}
% 在这里添加关于HPC集群理论设计的内容

\subsubsection{Software and Hardware Configurations}
% 在这里添加关于软件和硬件配置的内容

\subsubsection{Interconnection, Power Consumption, Performance Evaluation, and Architecture Analysis}
% 在这里添加关于互连、功耗、性能评估和架构分析的内容

\subsection{HPL and HPCG Benchmarks}

\subsubsection{Software Environment}
% 在这里添加关于软件环境的描述

\subsubsection{Performance Optimization and Testing Methods}
% 在这里添加关于性能优化和测试方法的内容

\subsubsection{Performance Measurement and Problem/Solution Analysis}
% 在这里添加关于性能测量和问题/解决方案分析的内容

\subsubsection{In-depth Analysis of HPL and HPCG Algorithms and Source Codes}
% 在这里添加关于HPL和HPCG算法和源代码的深入分析

\subsection{Optimization for AlphaFold3 Inference}

\subsection{GPU Inference Optimization}

\subsubsection{Model Deployment}
Since the program requires more than 18GB of video memory, we chose to deploy the model on the YSU HPC supercomputing cluster. Dependencies were installed according to the official dockerfile documentation. Due to the cluster's CUDA driver version being 12.0, which differs from the dockerfile's 12.6, we needed to select a different jax version. According to the \href{https://docs.jax.dev/en/latest/installation.html#nvidia-gpu}{jax installation documentation}, we used the following commands to install jax:

\begin{lstlisting}[language=bash]
pip install --upgrade pip
pip install --upgrade "jax[cuda12]"
\end{lstlisting}

After installation, the jax version is 0.5.0, which differs from the version required in the dockerfile but still runs normally.

\subsubsection{Hardware Configuration}
Using the compute01 node of the supercomputing cluster, which is configured with: CPU: Intel Xeon Gold 5218R @ 2.10GHz, GPU: 2* NVIDIA GeForce 3090 24G, RAM: 125G.

\subsubsection{Environment Variables Setup}
\begin{lstlisting}[language=bash]
export XLA_PYTHON_CLIENT_MEM_FRACTION=0.95
export JAX_TRACEBACK_FILTERING=off
\end{lstlisting}

\subsubsection{Program Execution Command}
\begin{lstlisting}[language=bash]
python ./run_alphafold.py \
? --input_dir=/input_dir \
? --output_dir=/output_dir \
? --model_dir=/model_dir \
? --norun_data_pipeline \
? --num_recycles=3 \
? --flash_attention_implementation=xla
\end{lstlisting}

\subsubsection{Program Results}
See cluster files for details.

\subsection{Program Optimization}

\subsubsection{Optimization Strategy Overview}
Through code analysis and observation of program execution results, we found that the model inference phase accounts for over 95\% of the total runtime. Therefore, we prioritized optimizing the model inference time. Additionally, we identified optimization opportunities in the feature extraction phase.

\subsubsection{Optimization Methods}
\textbf{Model Inference Phase:} Used jax compilation cache directory to cache compiled functions and model parameters, reducing compilation time during model inference. \\
\textbf{Feature Extraction Phase:} Defined \texttt{FeatureCache} class to cache feature data, reducing repeated computations and memory usage. Implemented \texttt{optimize\_features} function to optimize data types and memory layout, \texttt{compress\_features} function to compress feature data, and parallel processing of feature data to reduce runtime.

\subsubsection{Optimization Methods}
Defined \texttt{create\_model\_runner} function to configure jax environment, including disabling 64-bit operations and setting thread count. Implemented \texttt{\_post\_process\_result} function for optimizing result processing and data type conversion. Created \texttt{ModelRunner} class with \texttt{\_split\_batch}, \texttt{run\_inference}, and \texttt{\_merge\_results} functions for dynamic batch processing, parallel model inference, and parallel result merging. Implemented \texttt{NumericsHandler} class with \texttt{handle\_coordinate\_numerics}, \texttt{handle\_general\_numerics}, and \texttt{check\_output\_numerics} functions for detecting and handling NaN/Inf values. Developed \texttt{CacheManager} class with \texttt{\_get\_cache\_key}, \texttt{\_serialize\_value}, \texttt{\_deserialize\_value}, and \texttt{put} functions for cache management. Created \texttt{MemoryManager} class with \texttt{get\_memory\_usage}, \texttt{update}, \texttt{cleanup}, and \texttt{monitor} functions for memory management.

\subsubsection{Optimization Results}
\textbf{Before optimization:} Total runtime: 3651.96s, Model inference: 3353.36s, Feature extraction: 298.60s. \\
\textbf{After optimization:} Total runtime: 3149.68s, Model inference: 3042.80s, Feature extraction: 106.88s. \\
\textbf{Total improvement:} 13.7\%, Model inference improvement: 9.3\%, Feature extraction improvement: 66.9\%.

\subsection{CPU Inference Optimization}

\subsubsection{Model Deployment}
Since the CPU version requires over 100GB of memory for large inputs, we chose to deploy the model on the login node of the YSU HPC supercomputing cluster, with the same deployment process as the GPU version.

\subsubsection{Hardware Configuration}
Using the login node configured with: CPU: Intel Xeon Gold 5218R @ 2.10GHz, RAM: 125G.

\subsubsection{Environment Variables Setup}
\begin{lstlisting}[language=bash]
export MKL_DEBUG_CPU_TYPE=5
export MKL_ENABLE_INSTRUCTIONS=AVX2
export KMP_AFFINITY="granularity=fine,compact,1,0"
export MKL_DYNAMIC=FALSE
\end{lstlisting}
\begin{lstlisting}[language=python]
import os
os.environ['JAX_PLATFORMS'] = 'cpu'      	
os.environ['JAX_SKIP_ROCM_TESTS'] = '1'   	
os.environ['JAX_SKIP_TPU_TESTS'] = '1'  		
os.environ['JAX_LOG_COMPILES'] = '0'  		
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
\end{lstlisting}

\subsubsection{Program Execution Command}
Same as GPU version.

\subsubsection{Program Results}
See cluster files for details.

\subsection{Program Optimization}

\subsubsection{Optimization Strategy Overview}
Through code analysis, we identified optimization opportunities in memory usage and CPU communication. We also found NaN/Inf values during program execution that needed handling.

\subsubsection{Optimization Methods}
Defined create\_model\_runner function to configure jax environment, including disabling 64-bit operations and setting thread count. Implemented \_post\_process\_result function for optimizing result processing and data type conversion. Created ModelRunner class with \_split\_batch, run\_inference, and \_merge\_results functions for dynamic batch processing, parallel model inference, and parallel result merging. Implemented NumericsHandler class with handle\_coordinate\_numerics, handle\_general\_numerics, and check\_output\_numerics functions for detecting and handling NaN/Inf values. Developed CacheManager class with \_get\_cache\_key, \_serialize\_value, \_deserialize\_value, and put functions for cache management. Created MemoryManager class with get\_memory\_usage, update, cleanup, and monitor functions for memory management.

\subsubsection{Optimization Results}
Before optimization: Total runtime: 44102.1s
After optimization: Total runtime: 43929.5s
Total improvement: 0.4\%, potentially limited by memory bandwidth based on CPU usage during runtime.

\subsection{Program Execution Process}

\subsubsection{Input Processing}
First, receives input in JSON format describing target molecule composition and experimental conditions, then sets parameters such as cycle count and diffusion sample number based on command-line arguments.

\subsubsection{Feature Extraction}
Performs MSA, filters structure templates based on sequence similarity and publication date, loads CCD and RDKit for non-standard residue processing and small molecule ligand 3D conformation generation, then encodes these into multidimensional vectors.

\subsubsection{Model Inference}
Processes sequence and pairing features, generates atomic coordinates, then performs iterative optimization through multiple cycles, using previous prediction results as input for each iteration.

\subsubsection{Result Processing}
Decodes atomic coordinates from model output Frame Transforms, calculates local bond lengths and angles, normalizes results, performs confidence assessment, and finally outputs 3D structures as PDB files.

\newpage

\section{RNA m5C Modification Site Detection and Performance Optimization Challenge}

\subsubsection{Workflow Description}
% 在这里添加关于工作流描述文件的内容

\subsubsection{m5C Sites File}
% 在这里添加关于m5C位点文件的内容

\subsubsection{Software Packaging}
% 在这里添加关于将整个工作流打包成单个软件工具或容器的内容

\subsubsection{Performance Optimization}
% 在这里添加关于记录和提交从“cutseq”开始到工作流结束的时间的内容

\newpage

\section{Additional Materials}
% 在这里添加关于HPL输出文件、HPCG输出文件、AlphaFold3推理挑战所需文件和RNA m5C挑战所需文件的内容

\newpage

\appendix
\section{Additional Technical Details}
\subsection{Configuration Files}
Example of including configuration files:

\begin{lstlisting}[language=bash, caption=HPL Configuration File]
# Sample HPL.dat
HPL.out      output file name
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
29000        Ns
1            # of NBs
256          NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
\end{lstlisting}

\newpage

\section{References}
\begin{thebibliography}{9}
\bibitem{example}
Author, \textit{Title of the Book}, Publisher, Year.
\end{thebibliography}

\newpage

\end{document}