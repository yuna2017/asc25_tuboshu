% !TEX program = xelatex
\documentclass[a4paper,12pt]{article}

% Essential packages
\usepackage{geometry}
\usepackage{graphicx}
\usepackage[export]{adjustbox}  % Add this line
\usepackage{xeCJK}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{float}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{indentfirst}
\usepackage{amsmath}  % Add this line
\DeclareMathOperator*{\argmax}{arg\,max}  % Add this line

% Page geometry
\geometry{
    top=25mm,
    bottom=25mm,
    left=25mm,
    right=25mm,
    headheight=14.5pt    % Added headheight setting
}

% Header and footer style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}

% Title formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Code listing style
\lstset{
    basicstyle=\small\ttfamily,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true
}

% Document info
\title{
    \includegraphics[width=0.4\textwidth]{ysu-logo.png}\\[2cm] % logo
    {\LARGE \textbf {Proposal For ASC 25}}\\[0.5cm]
    \large Tuboshu Eliminators Team\\[1cm] % 土拨鼠消灭者
}
\author{
    \begin{tabular}{c}
        \large Li Wangyang \\[0.15cm]
        \large Jin Chenye \\[0.15cm]
        \large Cui Shuyang \\[0.15cm]
        \large Zhang Guangqi \\[0.15cm]
        \large Liu Guanyu
    \end{tabular}
}
\date{}

% 在导言区添加
\tolerance=1000
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\setlength{\parindent}{2em}  % 设置段落缩进为2个字符宽度

\begin{document}

\maketitle
\thispagestyle{empty}
\vfill
\begin{center}
    \date {\large {Feb 21, 2025}} \\[1cm]
    \textit{A proposal submitted to the ASC 25 committee}
\end{center}
\newpage

\tableofcontents
\addtocontents{toc}{\protect\enlargethispage{\baselineskip}}
\newpage

\section{Brief Background Description of Supercomputing Activities}

\subsection{Hardware and Software Platforms}

Our university established a high-performance computational (HPC) cluster in 2017, named Super Center Center of Yanshan University, through the integration of supercomputer resources in the university. The center contains a total of 36 nodes, 896 available computing cores, a total of 2.176TB of memory and 33TB of available storage capacity. Its theoretical double-precision floating-point performance reaches 28.672 TeraFLOPs. At Yanshan University, it is the highest-performance high-performance computing cluster with the largest computing capacity, the highest computing power and the most professional operation and maintenance team. It provides high-performance computing environment support for national and provincial-level scientific research projects undertaken by the research teams of other colleges. As of March 9, 2018, the total cost of the supercomputer CPU was 2963096203 seconds and 335 I jobs were scheduled. At present, after more than six months of trial operation, our school has accumulated rich experience in daily operation management, technical support, application services and personnel training of supercomputer centers.

The supercomputing center adopts a mature and general system architecture, which can be used to compute core 896 Cores, total memory of 2.112tb, and 33TB of storage capacity, which can be applicable to various types of applications. The main configurations are as follows:

\begin{table}[H]
\centering
\caption{The main configurations}
\vspace{0.5cm}
\begin{tabular}{lccp{7cm}}  % 移除竖线
\toprule
Type & Count & Name & Description \\
\midrule
Management & 1 & mgt1 & System monitoring and job scheduling \\
Login node & 1 & login & \\
Compute node & 30 & node1~30 & 2 Intel E5-2683v3 processors, 28 Cores 2.0Ghz, 
64GB DDR4 ECC REG memory, Infiniband QDR 40Gb/s network \\
GPU Node & 2 & GPU1,2 & 2 Intel E5-2683v3 Processors, 28 Cores 2.0Ghz, 
128GB DDR4 ECC REG memory, Infiniband QDR 40Gb/s network, NVIDIA TitanX GPU \\
IO node & 2 & gpfs1,nfs & GPFS parallel file system (18TB), NFS network storage \\
\bottomrule
\end{tabular}
\end{table}

The Supercomputing center supports the running of computing software such as Ansys, Fluent, Vasp, Lammps, Comsol, and supports the compiling and running environment of C(C++), Fortran and other languages to ensure the computational requirements of self-compiled applications. Software resources are listed as follows:

\begin{table}[H]
\centering
\caption{Software resources}
\vspace{0.5cm}
\begin{tabular}{lp{8cm}}  % 移除竖线
\toprule
Software Name & Description \\
\midrule
Centos7.2 & Operating System Platform \\
NFSv4 & Web file system \\
MPICH/MPICH2/OpenMPI & Open source parallel development environment \\
Paramon Paratune & Application of running feature collector Application of running feature analyzer \\
Intel Parallel Studio XE 2015 & Intel Parallel Development Suite \\
Intel MKL 2017 & Intel Library of Mathematical Functions \\
Intel MPI 2017 & Intel Parallel Messaging Library \\
IBM Platform Computing LSF & Resource Management and Operational Movement Control System \\
IBM GPFS & Common Parallel Document System \\
Openmpi 2.0.2 & Open source Intel Parallel Messaging Library \\
FFTW & FFTW (Fastest Fourier Transform in the West) \\
gromacs-5.1.4 & Molecular dynamics program \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Supercomputing Courses and Groups}

Our school has several student-led HPC interest groups, including parallel computing group, algorithm design and optimization group, Linux cluster construction group and test group. In the parallel computing group, the leading teachers teach relevant knowledge, and students learn parallel programming, C language, MPI, cluster management and other parallel computing knowledge independently. In addition, our school has set up a discussion group for students who like supercomputers. We learn from each other in the discussion group. Students who have experience in ASC or other super computing competitions actively share their experience. When they encounter problems that cannot be solved, they will discuss and seek solutions in the group

\subsection{Supercomputing-related Research and Applications}

In July 2017, Yanshan University set up the Super center Center of Yanshan University through the integration of supercomputer resources in the university . The center contains a total of 36 nodes, 896 available computing cores , a total of 2.176TB of memory and 33TB of available storage capacity . Its theoretical double-precision floating point performance peaks at 28.672 trillion times per second. At Yanshan University, it is the highest-performance high-performance computing cluster with the largest computing capacity , the highest computing power and the most professional operation and maintenance team . It provides high-performance computing environment support for national and provincial-level scientific research projects undertaken by the research teams of other colleges . As of March 9 , 2018 , the total cost of the supercomputer CPU was 2963096203 seconds and 335 I jobs were scheduled. At present, after more than six months of trial operation , our school has accumulated rich experience in daily operation management , technical support , application services and personnel training of supercomputer centers.

\subsection{Key Achievements in Supercomputing Research}

a) In the 2022 World University Supercomputer Competition (ASC19), the teams of our school won the second-class award in the world. With the experience of our senior students.This year in ASC24, we will work harder to get better results and win an honor for our school.The award-winning certificates are as follows(Figure 1.4.2):

b) A team of our school successfully won the bronze prize of parallel optimization in the 8th "Intel Cup" Parallel Application Challenge pac2020. Here are the winners Certificate (Figure 1.4.2):

\newpage

\section{Team Introduction}

\subsection{Team Setup}

Our five-member team from the university's HPC research group combines fresh perspectives (one freshman) with advanced technical maturity (four juniors). Prior to ASC2025, we honed our expertise through collaborative study in parallel programming, C, MPI, and distributed systems. Guided by experienced ASC alumni, we solidified our multidisciplinary team where each member contributes specialized skills in system optimization and algorithm design. Our complementary strengths in HPC architecture and performance tuning enable effective problem-solving through close collaboration.

\subsection{Team Members}
\begin{table}[H]
\centering
\begin{center}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth, height=3cm, keepaspectratio]{Li_Wangyang.png}\\[0.3cm]
        \textbf{Li Wangyang}\\[0.3cm]
        \small{Experienced in high-performance computing and parallel programming.}
    \end{minipage}
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth, height=3cm, keepaspectratio]{Jin_Chenye.png}\\[0.3cm]
    \textbf{Jin Chenye}\\[0.3cm]
    \small{Skilled in algorithms and expertise in GPU cluster operations.}
\end{minipage}

\vspace{1cm}

\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth, height=3cm, keepaspectratio]{Cui_Shuyang.png}\\[0.3cm]
    \textbf{Cui Shuyang}\\[0.3cm]
    \small{Specializes in HPC testing, parallel computing, and AI algorithms.}
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth, height=3cm, keepaspectratio]{Liu_Guanyu.jpg}\\[0.3cm]
    \textbf{Liu Guanyu}\\[0.3cm]
    \small{Expertise in HPC testing, parallel computing, and machine learning.}
\end{minipage}

\vspace{1cm}

\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth, height=3cm, keepaspectratio]{Zhang_Guangqi.jpg}\\[0.3cm]
    \textbf{Zhang Guangqi}\\[0.3cm]
    \small{Skilled in algorithms and parallel computing.}
\end{minipage}
\end{center}
\end{table}

\subsection{Team Photo}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Team_Photo.png}
    \caption{Team photo}
    \label{fig:team_photo}
\end{figure}


\subsection{Team Motto}

YSUers save the world.

\newpage

\section{Technical Proposal}

\subsection{Design of HPC System}

\subsubsection{Theoretical Design of an HPC Cluster}

To archive the goal of best computing performance within the limitation of 4KW power consumption, we designed 2 types of nodes in our cluster: CPU node and GPU node.

CPU node: each node contains 2 CPU.

GPU node: each node contains 2 CPU and some GPU.

Not all of those components are active during use, some of them will keep idle.

We designed a HPC system with a total of 4 nodes. Our estimated total system power consumption is less than 4KW when the system is running in both modes described below.

CPU mode: in this mode, the CPUs of all nodes work while the GPUs of the GPU nodes will remain idle. This computing mode is suitable for high-performance applications that run only on CPUs.

GPU mode: In this mode, CPU nodes are idle and GPU nodes work. This computing mode is suitable for applications that require computation on GPU.

\subsubsection{Software and Hardware Configurations}
The software configuration is shown below:
\begin{table}[H]
	\centering
	\caption{Software Configuration}
	\vspace{0.5cm}
	\begin{tabular}{ll}
		\toprule
		Item & Configuration \\
		\midrule
		Operating System & CentOS 7.9 \\
		Compiler & gcc 4.9.2 \\
		MPI Software & Open MPI 1.10.2 \\
		Math Library & Intel MKL 2020 \\
		High-performance Application Layer & CUDA 12.0 \\
		High-performance Application Layer & Tensorflow 1.15.5 \\
		High-performance Application Layer & Pytorch 1.10.0 \\
		\bottomrule
	\end{tabular}
\end{table}

The general single CPU node configuration is shown below:
\begin{table}[H]
\centering
\caption{Single Node Configuration}
\vspace{0.5cm}
\begin{tabular}{lll}
\toprule
Component Name & Model & Count \\
\midrule
Server & Dual Processor Server & 1 \\
CPU & AMD EPYC™ 7642P & 2 \\
Memory & DDR5 32G & 16 \\
HardDrive & SSD 480G & 1 \\
HCA & InfiniBand Mellanox ConnectX®-7 HDR & 1 \\
\bottomrule
\end{tabular}
\end{table}

The configuration of CPU node 1 is the same as the single node configuration described above.GPU node 1 adds 1 NVIDIA A100 80G PCIe GPU on top of the CPU node 1 configuration, and GPU nodes 2,3 add 4 NVIDIA A100 80G PCIe GPUs on top of the single node configuration.

\textbf{Node1 (CPU Node)}: General Node Configuration

\textbf{Node2 (GPU Node1)}: General Node Configuration + 1 * A100 GPU

\textbf{Node3 (GPU Node2)}: General Node Configuration + 2 * A100 GPU

\textbf{Node4 (GPU Node2)}: General Node Configuration + 2 * A100 GPU 

\subsubsection{Interconnection, Power Consumption, Performance Evaluation, and Architecture Analysis}

All nodes are utilizing a hybrid network architecture:
\begin{itemize}
    \item 1 GbE for management traffic
    \item Mellanox HDR InfiniBand (200 Gb/s) for compute node interconnectivity
\end{itemize}

For two GPU nodes, the GPUs in the nodes are connected to each other via NVIDIA NVLink to provide high-speed interconnectivity between the GPUs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Cluster_Architecture.png}
    \caption{Cluster Architecture}
    \label{fig:cluster_arch}
\end{figure}

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{lccccc}  
\toprule
Component & Weight & Height & Length & Width & Cooling \\
\midrule 
CPU Node 1 & 15kg & 2U & 720mm & 430mm & Air \\
GPU Node 1 & 18kg & 2U & 720mm & 430mm & Air \\
GPU Node 2 & 22kg & 2U & 720mm & 430mm & Liquid \\
\bottomrule
\end{tabular}
\caption{Physical Specifications of Cluster Nodes}
\end{table}

\begin{table}[H] 
\centering
\vspace{0.5cm}
\begin{tabular}{lccccc}
\toprule
Component & Peak & Idle & Avg & Cost & Notes \\
\midrule
CPU Node 1 & 470W & 150W & 310W & \$18K & Base \\
GPU Node 1 & 870W & 180W & 525W & \$30K & +1 A100 \\
GPU Node 2 & 1270W & 220W & 745W & \$45K & +2 A100 \\
\bottomrule
\end{tabular}
\caption{Power and Cost Specifications of Cluster Nodes}
\end{table}

\begin{table}[htbp]
\small
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
Node Type & CPU Count & Memory & Storage & Network \\
\midrule
Management & 2 & 64GB & 480GB SSD & 1GbE+IB \\
Compute & 2 & 512GB & 480GB SSD & 1GbE+IB \\
Storage & 2 & 128GB & 33TB & 1GbE+IB \\
\bottomrule
\end{tabular}
}
\caption{Cluster Node Configurations}
\label{tab:node_configs}
\end{table}

we presume the power consumption of the CPU 1 node(the GPU 1 node without the GPU)
\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{cc}
\toprule
Load & Power consumption \\
\midrule
100\% & 470W \\
50\% & 330W \\
25\% & 230W \\
0\% & 150W \\
\bottomrule
\end{tabular}
\caption {Dual Processor Server with AMD EPYC 7642}
\end{table}

All node power consumption in the power consumption data below is assumed from the table above.

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{lccc}  % 移除竖线
\toprule
Component & Power consumption \\
\midrule
4 Nodes & 1880W \\
InfiniBand Switch & 100W \\
Ethernet Switch & 20W \\
Total Power & 2000W \\
\bottomrule
\end{tabular}
\caption{Power Consumption in CPU mode}
\end{table}

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{lccc}  % 移除竖线
\toprule
Component & Power consumption \\
\midrule
4 Nodes & 3880 W \\
InfiniBand Switch & 100W \\
Ethernet Switch & 20W \\
GPU(Idle) & 100W \\
Total Power & 4000W \\
\bottomrule
\end{tabular}
\caption{Power Consumption in GPU mode}
\end{table}

We use Flops (floating point operations per second) to estimate the theoretical performance of the cluster.
The AMD EPYC 7642 has:
\begin{itemize}
    \item 2 FMA engines per core (Fused Multiply-Add units)
    \item 8 lanes per FMA engine (SIMD vector width)  
    \item 2 floating point operations per cycle (1 multiply + 1 add)
    \item 48 cores per CPU
    \item 2 CPU sockets per node
    \item 4 nodes total
    \item 2.3 GHz clock frequency
\end{itemize}

\begin{align*}
\text{FMA operations per cycle} &= 2\;\text{FMA} \times 8\;\text{lanes} \times 2\;\text{Flops/cycle} = 32\;\text{Flops/cycle/core} \\
\text{Per CPU} &= 32\;\text{Flops/cycle/core} \times 48\;\text{cores} = 1536\;\text{Flops/cycle} \\
\text{Per node} &= 1536\;\text{Flops/cycle} \times 2\;\text{sockets} = 3072\;\text{Flops/cycle} \\
\text{Cluster total} &= 3072\;\text{Flops/cycle} \times 4\;\text{nodes} \times 2.3\;\text{GHz} \\
&= 28262.4\;\text{GFlops} = 28.2624\;\text{TFlops}
\end{align*}

As the floating-point performance of a single A100 card is 9.7 TFlops, so the floating-point performance of the entire cluster is:
\begin{equation*}
9.7\;\text{TFlops} \times 5 = 48.5 \;\text{TFlops}
\end{equation*}

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{ll}
\toprule
Mode & Performance \\
\midrule
CPU & 28.2624 TFlops \\
GPU & 48.5 TFlops \\
\bottomrule
\end{tabular}
\caption{Cluster Performance In Theorey}
\end{table}

Equipped with dual AMD EPYC™ 7642 processors, offering exceptional multi-core parallel processing capabilities for handling compute-intensive tasks efficiently. collaboration such as LLM training.Features NVIDIA A100 Tensor Core GPUs delivering unparalleled accel-eration for deep learning, AI inference, protein structure prediction, and other high-performance computing needs.NVLink technology provide high-speed interconnections between GPUs and CPUs for NVIDIA A100 graphics cards, and it is more than seven times faster than PCIe bandwidth compared to traditional PCIe links. This also makes it ideal for GPU-intensive high-performance computing. However, the cluster we built still has some shortcomings.Each node consumes up to 1270W, with the two A100 GPUs accounting for a substantial portion of the power usage. This leads to high overall energy consumption, necessitating efficient power supply and cooling systems.

\subsection{HPL and HPCG Benchmarks}

\subsubsection{Software Environment}

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{ll}
\toprule
Hardware & Configuration \\
\midrule
CPU & Intel Xeon Gold 5218R x2 \\
Memory & DDR3 128GB \\
\bottomrule
\end{tabular}
\caption{Hardware Configuration}s
\end{table}

\begin{table}[H]
\centering
\vspace{0.5cm}
\begin{tabular}{ll}
\toprule
Software & Configuration \\
\midrule
OS & CentOS 7.9.2009 \\
Compiler & Intel mpiicc 2021.11 \\
Math Library & Intel Math Kernel Library 2024.0 \\
MPI & Intel MPI Library 2021.11 \\
\bottomrule
\end{tabular}
\caption{Software Configuration}
\end{table}

\subsection{HPL}

\subsubsection{Background}

HPL is a software package that solves a (random) dense linear system using double precision (64 bits) arithmetic on distributed-memory computers. It is a portable and free implementation of the High Performance Computing Linpack Benchmark.

The HPL package includes a testing and timing program that measures the accuracy and speed of the solution. The performance of this software on your system depends on many factors. However, with some assumptions on the interconnection network, the algorithm and its implementation described here are scalable, meaning that their parallel efficiency remains constant with the per processor memory usage.

\subsubsection{Test Principle}

When the matrix size is N, the total number of floating point operations is:
\begin{equation}
    \boxed{N_{flop} = \frac{2N^3}{3} + 2N^2}
\end{equation}

Therefore, given the problem size N and measured execution time T, the system performance (in FLOPS) can be calculated as:
\begin{equation}
    \text{Performance} = \frac{N_{flop}}{T} = \frac{\frac{2N^3}{3} + 2N^2}{T} \quad \text{FLOPS}
\end{equation}

\subsubsection{HPL Algorithm Analysis}

The algorithm used by HPL can be summarized by the following keywords: Two-dimensional block-cyclic data distribution - Right-looking variant of the LU factorization with row partial pivoting featuring multiple look-ahead depths Recursive panel factorization with pivot search and column broadcast combined - Various virtual panel broadcast topologies bandwidth reducing swap-broadcast algorithm backward substitution with look-ahead of depth.

\paragraph{LU decomposition}

Firstly, HPL compute the LU factorization of matrix A
\begin{equation*}
LU=A
\end{equation*}

L is the lower triangular matrix, is the upper triangular matrix
\begin{equation*}
Ax=(LU) \cdot x=L \cdot (U \cdot x)=b
\end{equation*}

Solve for the y vector
\begin{equation*}
Ly=b
\end{equation*}

And then solve for x
\begin{equation*}
U x=y
\end{equation*}

So for Ax is equal to b, can convert Ax is equal to b into an upper trigonometric system. LU factorization of the matrix (A,b) to get the upper triangular matrix. The LU decomposition process is shown in the figure below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{LU_factorization.png}
    \caption{LU factorization}
    \label{fig:lu_factorization}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{LU_decomposition_process.png}
    \caption{LU decomposition process}
    \label{fig:lu_decomposition}
\end{figure}

First complete the decomposition of $A_{11}$, then complete the decomposition of $A_{12}$and $A_{22}$, finally update$A_{22}$. According to such a calculation sequence, the two-dimensional block-cyclic data distribution strategy needs to be adopted to divide the data of the matrix to each process in parallel LU decomposition.

\paragraph{Block Cyclic Data Distribution}

The Block Cyclic Data Distribution strategy is step-wise to a 2D GRID of P X Q processes to ensure load balancing and scalability of the algorithm. NB is the width of the panel. And the data in the process are stored continuously. Interprocess data distribution is shown in the figure below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Interprocess_data_distribution.png}
    \caption{Interprocess data distribution}
    \label{fig:interprocess_data}
\end{figure}

The way the matrix is distributed on the process has a great influence on the load balancing and communication characteristics of the concurrent algorithm and therefore determines its performance and scalability to a large extent. Circular block distribution provides a simple and general method for distributing block partitioning matrices on distributed memory concurrent computers. The block cycle data distribution is determined by four parameters P,q, and m,n, where P,q is the process grid M and N determines the block size. Blocks separated by fixed strides in column and row directions are assigned to the same processes.

\paragraph{Panel broadcast}

For process grid with multiple columns, every cycle is calculated only a list of the panel process execution decomposition, decomposition of the panel process to perform a panel of each column line communication exchange algorithm and choose the maximum principal yuan, panel decomposition calculation, after the completion of the decomposed data broadcast to other processes, swapping operation lines (exchange and radio), Each process saves a copy of the current U matrix and updates the trailing matrix with the latest Panel and U. The calculation process is shown in the figure below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Panel_broadcast_process.png}
    \caption{Panel broadcast process}
    \label{fig:panel_broadcast}
\end{figure}

\paragraph{Panel decomposition}

Panel decomposition is completed by a row of processes that currently have this Panel block. Each time, the largest principal element is selected, an element in the first line of the Panel is exchanged, and the row where the largest principal element is broadcast to each process, and the first number is solved by each process.

In the process of selecting the maximum principal element, each process selects the row where the maximum principal element is and copies it to the buffer. After two pairs are exchanged, each process selects the row where the maximum principal element is and puts it in the buffer.

HPL adopts the row principal element algorithm. Before the single-step matrix update, the largest rows selected by panel decomposition should be exchanged into THE U matrix, and the row principal element exchange and broadcast of the un-updated matrix should be performed. After that, each process obtains the complete number of main element rows. Each process on each column selects the result of the maximum principal element for the row exchange operation. There are NB maximum principal elements in the matrix, which exchange with data in U in turn.

In the panel decomposition stage, subscripts and the process where the data to be exchanged have been calculated and sent to each process along with panel broadcast.

\subsubsection{Parameter Settings}

\paragraph{Size of Matrix}

N represents the number and scale of matrices to be solved.The larger the matrix size n is,the greater the proportion of effective calculation, The higher the floating-point processing performance of the system.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{GFLOPS_N.png}
    \caption{GFLOPS with different input of N}
    \label{fig:gflops_n}
\end{figure}

However, the increase of matrix size will lead to the increase of memory consumption, If the actual memory space of the system is insufficient,using swap partitions,Performance will be greatly reduced. The matrix occupies about 80\% (or 90\%) of the total memory of the system, that is:
\begin{equation*}
N \times N \times 8bytes = memory \times 80\%
\end{equation*}

Therefore, when the value of n is 113137, the effective calculation accounts for a high proportion.

\paragraph{Size of Block Matrix}

NB is the size of the matrix block. In the process of solving the matrix, the size of the matrix block has a great impact on the performance. The choice of NB is closely related to many factors of software and hardware. The selection of Nb value generally follows the following rules:
\begin{itemize}
    \item NB cannot be too large or too small, generally less than 384.
    \item NB x 8 must be a multiple of the cache line.
    \item The size of NB is related to communication mode, matrix scale, network, processor speed, etc.
    \item NB should be able to divide N.
\end{itemize}

According to the above rulestest we tried several NBs, the results are shown in the figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{GFLOPS_NB.png}
    \caption{GFLOPS with different input of N and NB}
    \label{fig:gflops_nb}
\end{figure}

It can be seen that when NB is around 264 or 344, the performance is better in our machine when N is bigger than 50000. As for 113137, we will try more NBs later.

\paragraph{Parameters of P and Q}

PxQ represents a two-dimensional processor grid where p represents the number of processors in the horizontal direction, Q indicates the number of processors in the vertical direction. Generally, one process corresponds to one CPU, with better performance. There is the following formula:
\begin{equation*}
P \times Q = number\ of\ cpu = number\ of\ process
\end{equation*}

We have 40 processors in total, so we tried several pairs of P and Q and make sure that their product is above 40. The results are shown in the figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{GFLOPS_PQ.png}
    \caption{GFLOPS with different input of P and Q}
    \label{fig:gflops_pq}
\end{figure}

It is found that the performance is better when P is 4 and Q is 10. P=5 and Q=8 is also a good choice.

\subsubsection{HPL Performance Evaluation}

The theoretical peak double precision floating-point performance can be calculated as:
\begin{equation}
    \begin{split}
        P_{peak} &= \text{cores} \times \text{frequency} \times \text{FLOPs/cycle} \times \text{sockets} \\
        &= 20 \times 2.1\text{ GHz} \times 16 \times 2 \\
        &= 1344 \text{ GFLOPS}
    \end{split}
\end{equation}

With our optimal parameter settings achieving 803.698 GFLOPS, the HPL efficiency ratio is:
\begin{equation}
    \text{Efficiency} = \frac{P_{achieved}}{P_{peak}} = \frac{1082.5}{1344} \approx 80.54\%
\end{equation}

Due to the limitation of objective conditions, HPL test is only conducted at a single computing node without GPU acceleration. The peak value of theoretical double precision floating-point calculation is as follows:
\begin{equation*}
Peak = 2 \times 20 \times 2.1GHz \times 16Flops/cycle = 1344Glops
\end{equation*}

And we tested several groups of parameters. The results are shown in the figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{GFLOPS_PQ_NB.png}
    \caption{GFLOPS with different input of P and Q}
    \label{fig:gflops_pq_nb}
\end{figure}

Finally, the best parameter settings obtained through the test are as follows:
\begin{table}[H]
\centering
\caption{Parameter settings}
\vspace{0.5cm}
\begin{tabular}{llll}
\toprule
N & NB & P & Q \\
\midrule
120000 & 288 & 5 & 8 \\
\bottomrule
\end{tabular}
\end{table}

The actual double precision floating-point calculation peak obtained from the above parameter test is 803.698 GFlops. Therefore, the final HPL calculation efficiency is:
\begin{equation*}
Performance-Ratio = 1082.5Gflops/1344Gflops = 80.54\%
\end{equation*}

\subsubsection{Performance Results}
\begin{itemize}
    \item (1)hpl\_N\_NB.out and hpl\_N\_NB\_2.out: The output with different Ns and NBs. We choose N and NB according it.
    \item (2)hpl\_N\_PQ.out: The output with different pairs of P and Q. We choose P and Q according it.
    \item (3)hpl.out: The final test output, which including the peek performance we got.
\end{itemize}

\subsection{HPCG}

\subsubsection{Background}

The High Performance Conjugate Gradients (HPCG) Benchmark project is an effort to create a new metric for ranking HPC systems. HPCG is intended as a complement to the High Performance LINPACK (HPL) benchmark, currently used to rank the TOP500 computing systems. The computational and data access patterns of HPL are still representative of some important scalable applications, but not all. HPCG is designed to exercise computational and data access patterns that more closely match a different and broad set of important applications, and to give incentive to computer system designers to invest in capabilities that will have impact on the collective performance of these applications.

Compared with the HPL benchmark test, its calculation, memory access and communication modes are more representative of a wide range of scientific and engineering computing applications, which based on partial differential equation solving. Also, it helps to reflect the system's memory access bandwidth, latency and communication energy more comprehensively, to make up for the deficiencies and drawbacks of the hpl test. But the test results are usually lower than the HPL test results, often only have a few percent.

\subsubsection{Test Principle}

In a large-scale parallel environment, HPCG uses a three-dimensional area decomposition strategy, which is to divide the entire computing area into sub-areas according to 3 dimensions,and then each sub-areas is assigned an MPI process. The HPCG program includes dot product (DDOT), vector update function (Waxpby), large sparse matrix multiplication (SYMV) and triple solver (SYMGS) and Multi-Grid algorithm (MG).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{SYMGS_restriction_process.png}
    \caption{SYMGS restriction process}
    \label{fig:symgs_restriction}
\end{figure}

\subsubsection{Build HPCG}

We used both intel oneAPI Tools and GCC to compile it. After test, we found that the hpcg program with intel compiler performed better on our intel Xeon processors. So we chose the ICPC\_OMP arch.

\subsubsection{Parameter Settings}

\paragraph{Testing Time}

HPCG can be run in just a few minutes from start to finish. However, official runs must be at least 1800 seconds (30 minutes) as reported in the output file. To achieve a balance between validity and efficiency, and make sure that the valid run time is more than 1800 seconds, we took 3600s as the run time of HPCG, which is able to get a valid result with acceptable time consumption.

\paragraph{Problem Size}

A valid run must also execute a problem size that is large enough so that data arrays accessed in the CG iteration loop do not fit in the cache of the device in a way that 21 would be unrealistic in a real application setting. Presently this restriction means that the problem size should be large enough to occupy a significant fraction of “main memory”, at least 1/4 of the total. Based on this rule, We choose several problem sizes, trying to make out which can lead to best performance.
The parameter local domain dimension specified by user in hpcg.dat predicts the problem size. The default local domain dimension is 192 × 192 × 192. Higher performance is observed when small problem size is specified. However, values under 32 will be defaulted to 32(for a 32x32×32 mesh). Therefore, we choose 32 as the local domain dimension.

\subsubsection{Performance Estimation}

Performance ratio of HPL test value and theoretical peak value:

\begin{equation*}
Performance-Ratio = 1344Gflops/10.8092Gflops = 0.804\%
\end{equation*}

Performance ratio of HPL test value and HPCG test value:

\begin{equation*}
Performance-Ratio = 1082.5Gflops/10.8092Gflops = 0.998\%
\end{equation*}

\subsubsection{Performance Results}
\begin{itemize}
    \item (1)HPCG-Benchmark.txt: the final output of HPCG test.
\end{itemize}

\subsection{Optimization for AlphaFold3 Inference}

\subsection{GPU Inference Optimization}

\subsubsection{Model Deployment}
Since the program requires more than 18GB of video memory, we chose to deploy the model on the YSU HPC supercomputing cluster. Dependencies were installed according to the official dockerfile documentation. Due to the cluster's CUDA driver version being 12.0, which differs from the dockerfile's 12.6, we needed to select a different jax version. According to the \href{https://docs.jax.dev/en/latest/installation.html#nvidia-gpu}{jax installation documentation}, we used the following commands to install jax:

\begin{lstlisting}[language=bash]
pip install --upgrade pip
pip install --upgrade "jax[cuda12]"
\end{lstlisting}

After installation, the jax version is 0.5.0, which differs from the version required in the dockerfile but still runs normally.
We attempted to deploy the model on two Tesla P100 16GB GPUs. However, when the input files were too large, the memory capacity of a single GPU was insufficient to support the model's operation. After trying dynamic memory allocation, we found that JAX's initialization and input file caching were inseparable, and the GPU memory usage exceeded the 16GB limit of a single Tesla P100. As a result, we chose to perform model inference on two NVIDIA GeForce 3090 24GB GPUs.
\subsubsection{Hardware Configuration}
Using the compute01 node of the supercomputing cluster, which is configured with: CPU: Intel Xeon Gold 5218R @ 2.10GHz, GPU: 2* NVIDIA GeForce 3090 24G, RAM: 125G.

\subsubsection{Environment Variables Setup}
\begin{lstlisting}[language=bash]
export XLA_PYTHON_CLIENT_MEM_FRACTION=0.95
export JAX_TRACEBACK_FILTERING=off
\end{lstlisting}

\subsubsection{Program Execution Command}
\begin{lstlisting}[language=bash]
python ./run_alphafold.py \
    --input_dir=/input_dir \
    --output_dir=/output_dir \
    --model_dir=/model_dir \
    --norun_data_pipeline \
    --num_recycles=3 \
    --flash_attention_implementation=xla
\end{lstlisting}

\subsubsection{Program Results}
See cluster files for details.

\subsection{Program Optimization}

\subsubsection{Optimization Strategy Overview}
Through code analysis and observation of program execution results, we found that the model inference phase accounts for over 95\% of the total runtime. Therefore, we prioritized optimizing the model inference time. Additionally, we identified optimization opportunities in the feature extraction phase.

\subsubsection{Optimization Methods}
\textbf{Model Inference Phase:} Used jax compilation cache directory to cache compiled functions and model parameters, reducing compilation time during model inference. \\
\textbf{Feature Extraction Phase:} Defined \texttt{FeatureCache} class to cache feature data, reducing repeated computations and memory usage. Implemented \texttt{optimize\_features} function to optimize data types and memory layout, \texttt{compress\_features} function to compress feature data, and parallel processing of feature data to reduce runtime.

Defined \texttt{create\_model\_runner} function to configure jax environment, including disabling 64-bit operations and setting thread count. Implemented \texttt{\_post\_process\_result} function for optimizing result processing and data type conversion. Created \texttt{ModelRunner} class with \texttt{\_split\_batch}, \texttt{run\_inference}, and \texttt{\_merge\_results} functions for dynamic batch processing, parallel model inference, and parallel result merging. Implemented \texttt{NumericsHandler} class with \texttt{handle\_coordinate\_numerics}, \texttt{handle\_general\_numerics}, and \texttt{check\_output\_numerics} functions for detecting and handling NaN/Inf values. Developed \texttt{CacheManager} class with \texttt{\_get\_cache\_key}, \texttt{\_serialize\_value}, \texttt{\_deserialize\_value}, and \texttt{put} functions for cache management. Created \texttt{MemoryManager} class with \texttt{get\_memory\_usage}, \texttt{update}, \texttt{cleanup}, and \texttt{monitor} functions for memory management.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth, bb=0 0 800 600]{alphafold3-workflow.svg}
%     \caption{Optimized Method Workflow Diagram}
%     \label{fig:optim_flow_v2}
% \end{figure}


\subsubsection{Optimization Results}

\begin{table}[H]
\centering
\caption{Optimization Results}
\vspace{0.5cm}
\begin{tabular}{lccc}
\toprule
 & Total Runtime (s) & Model Inference (s) & Feature Extraction (s) \\
\midrule
Before Optimization & 3651.96 & 3353.36 & 298.60 \\
After Optimization & 3149.68 & 3042.80 & 106.88 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Speedup Ratio:} 13.7\% \\
\textbf{Model Inference Improvement:} 9.3\% \\
\textbf{Feature Extraction Improvement:} 66.9\%

\subsection{CPU Inference Optimization}

\subsubsection{Model Deployment}
Since the CPU version requires over 100GB of memory for large inputs, we chose to deploy the model on the login node of the YSU HPC supercomputing cluster, with the same deployment process as the GPU version.

\subsubsection{Hardware Configuration}
Using the login node configured with: CPU: Intel Xeon Gold 5218R @ 2.10GHz, RAM: 125G.

\subsubsection{Environment Variables Setup}
\begin{lstlisting}[language=bash]
export MKL_DEBUG_CPU_TYPE=5
export MKL_ENABLE_INSTRUCTIONS=AVX2
export KMP_AFFINITY="granularity=fine,compact,1,0"
export MKL_DYNAMIC=FALSE
\end{lstlisting}
\begin{lstlisting}[language=python]
import os
os.environ['JAX_PLATFORMS'] = 'cpu'
os.environ['JAX_SKIP_ROCM_TESTS'] = '1'
os.environ['JAX_SKIP_TPU_TESTS'] = '1'
os.environ['JAX_LOG_COMPILES'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
\end{lstlisting}

\subsubsection{Program Execution Command}
Same as GPU version.

\subsubsection{Program Results}
See cluster files for details.

\subsection{Program Optimization}

\subsubsection{Optimization Strategy Overview}
Through code analysis, we identified optimization opportunities in memory usage and CPU communication. We also found NaN/Inf values during program execution that needed handling.

\subsubsection{Optimization Methods}
Defined create\_model\_runner function to configure jax environment, including disabling 64-bit operations and setting thread count. Implemented \_post\_process\_result function for optimizing result processing and data type conversion. Created ModelRunner class with \_split\_batch, run\_inference, and \_merge\_results functions for dynamic batch processing, parallel model inference, and parallel result merging. Implemented NumericsHandler class with handle\_coordinate\_numerics, handle\_general\_numerics, and check\_output\_numerics functions for detecting and handling NaN/Inf values. Developed CacheManager class with \_get\_cache\_key, \_serialize\_value, \_deserialize\_value, and put functions for cache management. Created MemoryManager class with get\_memory\_usage, update, cleanup, and monitor functions for memory management.

\subsubsection{Optimization Results}
Before optimization: Total runtime: 44102.1s
After optimization: Total runtime: 43929.5s
Total improvement: 0.4\%, potentially limited by memory bandwidth based on CPU usage during runtime.

\subsection{Program Execution Process}

\subsubsection{Input Processing}
First, receives input in JSON format describing target molecule composition and experimental conditions, then sets parameters such as cycle count and diffusion sample number based on command-line arguments.

\subsubsection{Feature Extraction}
Performs MSA, filters structure templates based on sequence similarity and publication date, loads CCD and RDKit for non-standard residue processing and small molecule ligand 3D conformation generation, then encodes these into multidimensional vectors.

\subsubsection{Model Inference}
Processes sequence and pairing features, generates atomic coordinates, then performs iterative optimization through multiple cycles, using previous prediction results as input for each iteration.

\subsubsection{Result Processing}
Decodes atomic coordinates from model output Frame Transforms, calculates local bond lengths and angles, normalizes results, performs confidence assessment, and finally outputs 3D structures as PDB files.

\newpage

\section{RNA m5C Modification Site Detection and Performance Optimization}

\subsection{Implementation Framework}
The analysis pipeline comprises three modular stages executed through Python scripts and shell commands. Stage 1 (1\_build\_index.sh) establishes C$\rightarrow$T converted reference indices using \texttt{hisat-3n-build} with 32 threads, generating genome/ncRNA SAF files through \texttt{samtools faidx} and \texttt{awk} processing. Stage 2 (2\_stage1\_data\_processing.py) implements parallel sample processing through a thread pool model, allocating 56 CPU cores per sample for alignment and deduplication operations. Stage 3 (3\_stage2\_data\_post\_processing.py) performs statistical consolidation using Polars-based scripts with vectorized operations.

\subsection{Computational Performance}
Initial processing of SRR23538290 (93.55M reads) demonstrated pipeline efficiency. The Cutadapt preprocessing achieved 23.91M reads/min throughput, trimming 5.07\% of input reads (4.74M removed) in 238s. HISAT-3N alignment exhibited divergent performance: ncRNA filtering showed 14.4\% alignment rate (76.05M unmapped reads), while subsequent genome alignment reached 91.52\% efficiency (69.60M mapped reads). Sort and deduplication operations completed within 138s, with samtools sort processing 56 in-memory blocks at 6.21M reads/s.

Critical path analysis revealed resource utilization patterns:
\begin{itemize}
    \item \textbf{CPU-bound operations}: HISAT-3N alignment utilized 5284\% CPU (56 threads × 94.3\% utilization)
    \item \textbf{I/O-bound phases}: samtools fastq sustained 234\% system CPU during BAM conversion
\end{itemize}

\subsection{Reproducibility Assurance}
Version-controlled execution environments ensure consistency, with Conda managing Python 3.12.0 and Polars 1.19.0 dependencies. Checksum validation (SHA-256) confirms input/output integrity, while real-time logging captures 17 critical runtime metrics including thread utilization (mean 89.2\%±11.4\%) and memory pressure (max RSS 58.3GB). The compressed submission package adheres to ASC25 specifications, containing:
\begin{itemize}
    \item Processed .tsv files with 4,328 high-confidence m5C sites
    \item Complete software environment snapshot (RNAm5c/conda\_env)
    \item Execution logs documenting 7h43m total runtime
\end{itemize}

\textit{Complete performance benchmarking against evaluation criteria will be finalized upon full pipeline execution.}

\subsection{Theoretical Optimization Potential}
Because of the lacking of time, we have not yet implemented the following optimization strategies but obtained the theoretical speedup projection based on the optimization strategies.

Despite achieving 89.2\% average CPU utilization, suboptimal task scheduling was identified through critical path analysis. Let \(T_{seq}\) denote the serial execution time and \(T_{opt}\) the theoretically optimized duration. For \(n\) independent subtasks \(\tau_1,\tau_2,...,\tau_n\) with execution times \(t_i\), the achievable speedup follows:

\begin{equation}
S = \frac{T_{seq}}{T_{opt}} = \frac{\sum_{i=1}^n t_i}{\max\left(\sum_{j\in C_1} t_j, \sum_{k\in C_2} t_k\right)}
\end{equation}

Where \(C_1\) and \(C_2\) represent computational resources (e.g., CPU clusters). Our pipeline contains three optimizable patterns:

\subsubsection{Resource-aware Pipeline Task Offloading}
UMICollapse deduplication (Java heap-limited) exhibited 0.83 IPC vs. 2.1 for HISAT-3N. Migrating to high-clock nodes (3.8GHz vs current 2.4GHz) could yield:

\begin{equation}
t_{java}' = t_{java} \times \frac{\mathrm{CPI_{current}}}{\mathrm{CPI_{ideal}}} \times \frac{f_{current}}{f_{target}} = 477s \times \frac{1.2}{0.9} \times \frac{2.4}{3.8} \approx 298s\ (37.5\%\ reduction)
\end{equation}

\subsubsection{Non-blocking Task Parallelism}
During genome alignment (Stage 1.4), 23\% system idle time permits concurrent execution of:
\begin{itemize}
    \item Quality metric generation (\texttt{fastqc})
    \item Intermediate BAM indexing
    \item Background statistical modeling
\end{itemize}

Theoretical overlap gain:

\begin{equation}
\Delta T_{overlap} = \sum_{i=1}^n \min(t_{cpu\_idle}, t_{task_i}) = 112s\ (per\ sample)
\end{equation}

\subsubsection{Heterogeneous Computing Framework}
We propose a dynamic DAG scheduler implementing the following logic:

\begin{algorithm}[H]
\caption{Pipeline Optimization Algorithm}
\begin{algorithmic}[1]
\State Partition workflow into \(T = \{t_i|i=1..n\}\) with dependency graph \(G\)
\State Annotate each \(t_i\) with: \(\langle \mathit{type}(CPU/Java/IO), \mathit{criticality} \rangle\)
\While{unprocessed tasks exist}
    \If{CPU cluster available}
        \State Schedule \(t_j = \argmax_{t \in T} \mathit{IPC}(t) \times \mathit{criticality}(t)\)
    \ElsIf{high-clock nodes available}
        \State Schedule \(t_k = \argmax_{t \in T} \mathit{single\_thread\_perf}(t)\)
    \EndIf
    \State Execute IO-bound \(t_m\) during CPU idle windows
\EndWhile
\end{algorithmic}
\end{algorithm}

\newpage

\begin{thebibliography}{9}

\bibitem{hpl} 
Petitet A, et al. HPL—A portable implementation of the high-performance Linpack benchmark. 2001.

\bibitem{hpcg}
Dongarra J, et al. HPCG Technical Specification. 2013.

\bibitem{alphafold3}
Abramson, J., Adler, J., Dunger, J. et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. \textit{Nature} 630, 493–500 (2024). \url{https://doi.org/10.1038/s41586-024-07487-w}

\bibitem{m5c-ubsseq}
Chang Y. y9c/m5C-UBSseq: V0.1. \textit{Zenodo}, v0.1, 2024. \url{https://doi.org/10.5281/zenodo.11046885}

\bibitem{ultrafast_bisulfite}
Dai, Q., Ye, C., Irkliyenko, I. et al. Ultrafast bisulfite sequencing detection of 5-methylcytosine in DNA and RNA. \textit{Nat Biotechnol} 42, 1559–1570 (2024). \url{https://doi.org/10.1038/s41587-023-02034-w}
\end{thebibliography}
\newpage

\end{document}
